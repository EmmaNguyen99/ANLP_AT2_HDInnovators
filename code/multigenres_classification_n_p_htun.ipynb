{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU, using /device:CPU:0.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if len(device_name) > 0:\n",
    "    print(\"Found GPU at: {}\".format(device_name))\n",
    "else:\n",
    "    device_name = \"/device:CPU:0\"\n",
    "    print(\"No GPU, using {}.\".format(device_name))\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/holys/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/holys/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/holys/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/holys/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras_tuner as kt\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tensorflow deep learning \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Flatten, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# RandomForest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ignore warnings \n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to transform valence sentiment labels\n",
    "def create_genre_label(genre):\n",
    "   if genre == 'blues':\n",
    "     return 1\n",
    "   elif genre == 'country':\n",
    "     return 2\n",
    "   elif genre == 'jazz':\n",
    "     return 3\n",
    "   elif genre == 'pop':\n",
    "     return 4\n",
    "   elif genre == 'reggae':\n",
    "     return 5\n",
    "   else:\n",
    "     return 0\n",
    "\n",
    "# def create_sentiment_from_valence(valence):\n",
    "#    if valence < 0.5:\n",
    "#      return 0\n",
    "#    else:\n",
    "#      return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lyrics_all_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lyrics'] = df['lyrics'].apply(str)\n",
    "\n",
    "# Convert all string of lyrics to lowercase.\n",
    "df['lyrics'] = df['lyrics'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text from lyrics. \n",
    "df['tokenized'] = df['lyrics'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All punctuations\n",
    "punc_marks = list(string.punctuation)\n",
    "# Remove all punctuations.\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [word for word in x if word not in punc_marks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jazz       2000\n",
       "pop        2000\n",
       "country    2000\n",
       "blues      2000\n",
       "rock       2000\n",
       "reggae     1978\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All stopwords of nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Customized list of stop words.\n",
    "stopwords.extend([\"'m\", \"'s\", \"'d\", \"hi\", \"im\", \"wa\", \"n't\",'wan','na','u','gon' ,'ahah','ayo',\"'get\", \"'ll\", \"'re\", \"'ve\", \"get\", \"still\", \"mmm\", \"ooh\", \"oooh\", \"yah\", \"yeh\",\"mmm\", \"hmm\",\"i'm\"])\n",
    "\n",
    "# Remove the stop words from the dataset and save the result to new column. \n",
    "df['cleaned_stopwords'] = df['tokenized'].apply(lambda x: [word for word in x if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>len</th>\n",
       "      <th>valence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cleaned_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>lyrics_lemmatized</th>\n",
       "      <th>lyrics_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>santana</td>\n",
       "      <td>wham!</td>\n",
       "      <td>1978</td>\n",
       "      <td>blues</td>\n",
       "      <td>cold chamber smoke kush gettin higher plane sw...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.857791</td>\n",
       "      <td>[cold, chamber, smoke, kush, gettin, higher, p...</td>\n",
       "      <td>[cold, chamber, smoke, kush, gettin, higher, p...</td>\n",
       "      <td>[(cold, a), (chamber, n), (smoke, v), (kush, n...</td>\n",
       "      <td>[cold, chamber, smoke, kush, gettin, high, pla...</td>\n",
       "      <td>cold chamber smoke kush gettin high plane swan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marvin sease</td>\n",
       "      <td>show me what you got</td>\n",
       "      <td>1991</td>\n",
       "      <td>blues</td>\n",
       "      <td>public service announcement weezy baby best ra...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.794930</td>\n",
       "      <td>[public, service, announcement, weezy, baby, b...</td>\n",
       "      <td>[public, service, announcement, weezy, baby, b...</td>\n",
       "      <td>[(public, a), (service, n), (announcement, n),...</td>\n",
       "      <td>[public, service, announcement, weezy, baby, b...</td>\n",
       "      <td>public service announcement weezy baby best ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the robert cray band</td>\n",
       "      <td>1040 blues</td>\n",
       "      <td>1993</td>\n",
       "      <td>blues</td>\n",
       "      <td>gotta rapper today forget fuck smokin brain ce...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.613561</td>\n",
       "      <td>[got, ta, rapper, today, forget, fuck, smokin,...</td>\n",
       "      <td>[got, ta, rapper, today, forget, fuck, smokin,...</td>\n",
       "      <td>[(got, v), (ta, a), (rapper, n), (today, n), (...</td>\n",
       "      <td>[get, ta, rapper, today, forget, fuck, smokin,...</td>\n",
       "      <td>get ta rapper today forget fuck smokin brain c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_name            track_name  release_date  genre  \\\n",
       "0               santana                 wham!          1978  blues   \n",
       "1          marvin sease  show me what you got          1991  blues   \n",
       "2  the robert cray band            1040 blues          1993  blues   \n",
       "\n",
       "                                              lyrics  len   valence  \\\n",
       "0  cold chamber smoke kush gettin higher plane sw...  198  0.857791   \n",
       "1  public service announcement weezy baby best ra...  198  0.794930   \n",
       "2  gotta rapper today forget fuck smokin brain ce...  198  0.613561   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [cold, chamber, smoke, kush, gettin, higher, p...   \n",
       "1  [public, service, announcement, weezy, baby, b...   \n",
       "2  [got, ta, rapper, today, forget, fuck, smokin,...   \n",
       "\n",
       "                                   cleaned_stopwords  \\\n",
       "0  [cold, chamber, smoke, kush, gettin, higher, p...   \n",
       "1  [public, service, announcement, weezy, baby, b...   \n",
       "2  [got, ta, rapper, today, forget, fuck, smokin,...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(cold, a), (chamber, n), (smoke, v), (kush, n...   \n",
       "1  [(public, a), (service, n), (announcement, n),...   \n",
       "2  [(got, v), (ta, a), (rapper, n), (today, n), (...   \n",
       "\n",
       "                                   lyrics_lemmatized  \\\n",
       "0  [cold, chamber, smoke, kush, gettin, high, pla...   \n",
       "1  [public, service, announcement, weezy, baby, b...   \n",
       "2  [get, ta, rapper, today, forget, fuck, smokin,...   \n",
       "\n",
       "                                      lyrics_cleaned  \n",
       "0  cold chamber smoke kush gettin high plane swan...  \n",
       "1  public service announcement weezy baby best ra...  \n",
       "2  get ta rapper today forget fuck smokin brain c...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to attain part of speech of words.\n",
    "def determine_wordnet_speech(word_tag):\n",
    "    if word_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif word_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif word_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Add part of speech tags and save the result to new column.\n",
    "df['pos_tags'] = df['cleaned_stopwords'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "# Function to get part of speech in WordNet format.\n",
    "df['pos_tags'] = df['pos_tags'].apply(lambda x: [(word, determine_wordnet_speech(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "# Lemmatize words and save the result to new column.\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "df['lyrics_lemmatized'] = df['pos_tags'].apply(lambda x: [word_lemmatizer.lemmatize(word, tag) for word, tag in x])\n",
    "\n",
    "# Convert list to string datatype. \n",
    "df['lyrics_cleaned'] = [' '.join(map(str,l)) for l in df['lyrics_lemmatized']]\n",
    "\n",
    "# Check few rows.\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform valence to sentiment labels\n",
    "df['genre'] = df['genre'].apply(create_genre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels \n",
    "y = df['genre']\n",
    "# Extract independent variables\n",
    "X = df['lyrics_cleaned']\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "# Create 3 cv for cross-validation \n",
    "cv = StratifiedKFold(n_splits=3, random_state=8, shuffle=True).split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.2, max_df=0.7, max_features=100)\n",
    "# Learn a vocabulary dictionary of all tokens in the training set.\n",
    "vectorizer.fit(X_train)\n",
    "# Transform training set and testing set to document-term matrix.\n",
    "X_train_count_vectorized = vectorizer.transform(X_train)\n",
    "X_test_count_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# Learn vocabulary and idf from training set.\n",
    "tfidf_transformer.fit(X_train_count_vectorized)\n",
    "# Transform a train and set count matrix to a tf-idf representation.\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_count_vectorized)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(random_state=8, n_estimators=50).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.58"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([estimator.tree_.max_depth for estimator in rf1.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [50, 100, 150],\n",
       " 'max_depth': [40, 55, 60],\n",
       " 'min_samples_split': [2, 4, 8]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [40, 55, 60],\n",
    "    'min_samples_split': [2, 4, 8]\n",
    "    }\n",
    "hyperparams_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END .max_depth=40, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=40, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=40, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=40, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=40, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=40, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=40, min_samples_split=2, n_estimators=150; total time=   5.6s\n",
      "[CV] END max_depth=40, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END max_depth=40, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END .max_depth=40, min_samples_split=4, n_estimators=50; total time=   2.0s\n",
      "[CV] END .max_depth=40, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END .max_depth=40, min_samples_split=4, n_estimators=50; total time=   1.7s\n",
      "[CV] END max_depth=40, min_samples_split=4, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=40, min_samples_split=4, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=40, min_samples_split=4, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=40, min_samples_split=4, n_estimators=150; total time=   5.2s\n",
      "[CV] END max_depth=40, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END max_depth=40, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END .max_depth=40, min_samples_split=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END .max_depth=40, min_samples_split=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END .max_depth=40, min_samples_split=8, n_estimators=50; total time=   1.7s\n",
      "[CV] END max_depth=40, min_samples_split=8, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=40, min_samples_split=8, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=40, min_samples_split=8, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=40, min_samples_split=8, n_estimators=150; total time=   4.9s\n",
      "[CV] END max_depth=40, min_samples_split=8, n_estimators=150; total time=   4.9s\n",
      "[CV] END max_depth=40, min_samples_split=8, n_estimators=150; total time=   4.9s\n",
      "[CV] END .max_depth=55, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=55, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=55, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=55, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=55, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=55, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=55, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END max_depth=55, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END max_depth=55, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END .max_depth=55, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END .max_depth=55, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END .max_depth=55, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=55, min_samples_split=4, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=55, min_samples_split=4, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=55, min_samples_split=4, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=55, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END max_depth=55, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END max_depth=55, min_samples_split=4, n_estimators=150; total time=   5.8s\n",
      "[CV] END .max_depth=55, min_samples_split=8, n_estimators=50; total time=   1.8s\n",
      "[CV] END .max_depth=55, min_samples_split=8, n_estimators=50; total time=   1.7s\n",
      "[CV] END .max_depth=55, min_samples_split=8, n_estimators=50; total time=   1.7s\n",
      "[CV] END max_depth=55, min_samples_split=8, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=55, min_samples_split=8, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=55, min_samples_split=8, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=55, min_samples_split=8, n_estimators=150; total time=   4.9s\n",
      "[CV] END max_depth=55, min_samples_split=8, n_estimators=150; total time=   4.9s\n",
      "[CV] END max_depth=55, min_samples_split=8, n_estimators=150; total time=   5.0s\n",
      "[CV] END .max_depth=60, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=60, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=60, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=60, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=60, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=60, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=60, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END max_depth=60, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END max_depth=60, min_samples_split=2, n_estimators=150; total time=   5.7s\n",
      "[CV] END .max_depth=60, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END .max_depth=60, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END .max_depth=60, min_samples_split=4, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=60, min_samples_split=4, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=60, min_samples_split=4, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=60, min_samples_split=4, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=60, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END max_depth=60, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END max_depth=60, min_samples_split=4, n_estimators=150; total time=   5.3s\n",
      "[CV] END .max_depth=60, min_samples_split=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END .max_depth=60, min_samples_split=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END .max_depth=60, min_samples_split=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END max_depth=60, min_samples_split=8, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=60, min_samples_split=8, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=60, min_samples_split=8, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=60, min_samples_split=8, n_estimators=150; total time=   5.1s\n",
      "[CV] END max_depth=60, min_samples_split=8, n_estimators=150; total time=   5.3s\n",
      "[CV] END max_depth=60, min_samples_split=8, n_estimators=150; total time=   5.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=8),\n",
       "             param_grid={&#x27;max_depth&#x27;: [40, 55, 60],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 8],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=8),\n",
       "             param_grid={&#x27;max_depth&#x27;: [40, 55, 60],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 8],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=8)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=8)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=8),\n",
       "             param_grid={'max_depth': [40, 55, 60],\n",
       "                         'min_samples_split': [2, 4, 8],\n",
       "                         'n_estimators': [50, 100, 150]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=8), hyperparams_grid, cv=3, verbose=2, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 40, 'min_samples_split': 8, 'n_estimators': 150}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_search_rf.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2521432402924937"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(random_state=8, max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'], n_estimators=best_params['n_estimators']).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = rf_best.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25242070116861437"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_preds, y_test)\n",
    "#ConfusionMatrixDisplay.from_estimator(rf_best, y_test_preds, y_test, cmap=\"Blues\", normalize='true')\n",
    "#plt.title(\"RFC with Count Vectorizer\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40420"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the length of our vocabulary\n",
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "# Create a dict of word and index from the list of sentences. Required before texts_to_sequences\n",
    "word_tokenizer.fit_on_texts(X)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "all_lyrics = X.values\n",
    "longest_lyrics = max(all_lyrics, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_lyrics))\n",
    "\n",
    "# texts_to_sequences: Transforms each text in texts to a sequence of integers (integers = index of word by fit_on_texts)\n",
    "padded_lyrics = pad_sequences(\n",
    "    word_tokenizer.texts_to_sequences(all_lyrics),\n",
    "    length_long_sentence, \n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_lyrics, y, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 1 4 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  with tf.device(device_name):\n",
    "    model = Sequential()\n",
    "    # embedding\n",
    "    hp_vector_size = hp.Int('vector_size', \n",
    "                            min_value=50, \n",
    "                            max_value=200, \n",
    "                            step=50)\n",
    "    model.add(\n",
    "        Embedding(input_dim=vocab_length,\n",
    "                  output_dim=hp_vector_size,\n",
    "                  input_length=length_long_sentence))\n",
    "\n",
    "    # first lstm\n",
    "    hp_lstm_units1 = hp.Int('lstm_units1', \n",
    "                            min_value=100, \n",
    "                            max_value=101, \n",
    "                            step=1)\n",
    "    model.add(LSTM(hp_lstm_units1, return_sequences=True))\n",
    "\n",
    "    # drop out\n",
    "    hp_dropout_rate = hp.Float('dropout_rate', \n",
    "                               min_value=0.4, \n",
    "                               max_value=0.8, \n",
    "                               step=0.2)\n",
    "    model.add(Dropout(hp_dropout_rate))\n",
    "\n",
    "    # second lstm\n",
    "    hp_lstm_units2 = hp.Int('lstm_units2', \n",
    "                            min_value=100, \n",
    "                            max_value=101, \n",
    "                            step=1)\n",
    "    model.add(LSTM(hp_lstm_units2))\n",
    "\n",
    "    # dense layer\n",
    "    # model.add(Dense(units = 100, activation='relu'))\n",
    "    model.add(Dense(units = 50, activation='relu'))\n",
    "    model.add(Dense(units = 25, activation='relu'))\n",
    "    model.add(Dense(6,activation='softmax'))\n",
    "    \n",
    "    # compile\n",
    "    hp_learning_rate = hp.Choice('learning_rate', \n",
    "                                 values=[5e-1])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 Complete [00h 03m 14s]\n",
      "val_accuracy: 0.16694490611553192\n",
      "\n",
      "Best val_accuracy So Far: 0.16694490611553192\n",
      "Total elapsed time: 02h 32m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     factor=3,\n",
    "                     max_epochs=100,\n",
    "                     directory=\"model_trials_1\",\n",
    "                     project_name=\"emotion_detector_1\",\n",
    "                     overwrite = True\n",
    "                     )\n",
    "                     \n",
    "stop_early = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "tuner.search(X_train, \n",
    "             y_train, \n",
    "             epochs=50,\n",
    "             validation_data=(X_test, y_test), \n",
    "             callbacks=[stop_early]\n",
    "             )\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
