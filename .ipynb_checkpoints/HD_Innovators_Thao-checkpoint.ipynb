{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlI9QnDVnagQ"
   },
   "source": [
    "# Applied Natural Language Processing - AT2 - HDInnovators\n",
    "__Content:__\n",
    "\n",
    "1. Import packages and create functions\n",
    "2. Load and explore the dataset\n",
    "3. Data preparation\n",
    "4. Genres classification\n",
    "  \n",
    "  4.1. Bert\n",
    "5. Sentiment analysis\n",
    "\n",
    "  5.1. RandomForest \n",
    "  \n",
    "  5.2. Neural network\n",
    "\n",
    "  5.3 Bert\n",
    "\n",
    "6. Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3y_IWgbLbCCc"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPzq-Phyol5z"
   },
   "source": [
    "## 1. Import packages and create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97eSUJ6XZ_cU",
    "outputId": "468a6ce2-884e-48aa-c850-b3d45c437c58"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6l/58wb3q7j51nc7n5zmk17nbcw0000gn/T/ipykernel_54756/3985453706.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found GPU at: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if len(device_name) > 0:\n",
    "#     print(\"Found GPU at: {}\".format(device_name))\n",
    "# else:\n",
    "#     device_name = \"/device:CPU:0\"\n",
    "#     print(\"No GPU, using {}.\".format(device_name))\n",
    "# import locale\n",
    "# def getpreferredencoding(do_setlocale = True):\n",
    "#     return \"UTF-8\"\n",
    "# locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GpgXL1FH_8C",
    "outputId": "afdc9a5b-3730-4d24-c332-8480616b366f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nguyenthao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nguyenthao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nguyenthao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nguyenthao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6l/58wb3q7j51nc7n5zmk17nbcw0000gn/T/ipykernel_54756/1887181680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# tensorflow vectorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# # drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# tensorflow vectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tensorflow deep learning models\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# ignore warnings \n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dm_00T1BQnsz"
   },
   "outputs": [],
   "source": [
    "# Create a function to transform valence sentiment labels\n",
    "def create_sentiment_from_valence(valence):\n",
    "   if valence >= 0 and valence < 1/3:\n",
    "     return 0\n",
    "   elif valence >= 1/3 and valence <  2/3:\n",
    "     return 2\n",
    "   else:\n",
    "     return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FufJTQrwo89L"
   },
   "source": [
    "## 2. Load and explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zep39NepWLE"
   },
   "source": [
    "__Load dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cG46E1rbIE2n"
   },
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "df = pd.read_csv('./Dataset with 2k each genre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "DR5of-4yrbwe",
    "outputId": "b1af4057-8288-44fc-8fd4-231582f105ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-147b9b31-ad42-402f-b1e1-d2b4754db6a7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>len</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13923</td>\n",
       "      <td>santana</td>\n",
       "      <td>wham!</td>\n",
       "      <td>1978</td>\n",
       "      <td>blues</td>\n",
       "      <td>cold chamber smoke kush gettin higher plane sw...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.857791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14958</td>\n",
       "      <td>marvin sease</td>\n",
       "      <td>show me what you got</td>\n",
       "      <td>1991</td>\n",
       "      <td>blues</td>\n",
       "      <td>public service announcement weezy baby best ra...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.794930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15045</td>\n",
       "      <td>the robert cray band</td>\n",
       "      <td>1040 blues</td>\n",
       "      <td>1993</td>\n",
       "      <td>blues</td>\n",
       "      <td>gotta rapper today forget fuck smokin brain ce...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.613561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15691</td>\n",
       "      <td>carl sims</td>\n",
       "      <td>it's just a party</td>\n",
       "      <td>2004</td>\n",
       "      <td>blues</td>\n",
       "      <td>girls knock ghetto ballin real thugs unngghhhh...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.705276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16322</td>\n",
       "      <td>rufus thomas</td>\n",
       "      <td>sixty minute man</td>\n",
       "      <td>2011</td>\n",
       "      <td>blues</td>\n",
       "      <td>diddle devil yabba double walk feel body feel ...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.651690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11973</th>\n",
       "      <td>26756</td>\n",
       "      <td>newsboys</td>\n",
       "      <td>your love never fails</td>\n",
       "      <td>2011</td>\n",
       "      <td>rock</td>\n",
       "      <td>ohhh separate away fail know mistake cause fai...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.550701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>26866</td>\n",
       "      <td>mumford &amp; sons</td>\n",
       "      <td>where are you now</td>\n",
       "      <td>2012</td>\n",
       "      <td>rock</td>\n",
       "      <td>come hear walk city streets say word finally e...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.101401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>26875</td>\n",
       "      <td>snow patrol</td>\n",
       "      <td>just say yes</td>\n",
       "      <td>2013</td>\n",
       "      <td>rock</td>\n",
       "      <td>run ways want stay okay pretend tell today han...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.236397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>27024</td>\n",
       "      <td>starset</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>2014</td>\n",
       "      <td>rock</td>\n",
       "      <td>life know inside beast grow wait chew rope cha...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.025556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>23435</td>\n",
       "      <td>ronnie self</td>\n",
       "      <td>bop-a-lena</td>\n",
       "      <td>1955</td>\n",
       "      <td>rock</td>\n",
       "      <td>gogalgo bopalena bopalena bopalena bopalena ye...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.968054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11978 rows Ã— 8 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-147b9b31-ad42-402f-b1e1-d2b4754db6a7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-147b9b31-ad42-402f-b1e1-d2b4754db6a7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-147b9b31-ad42-402f-b1e1-d2b4754db6a7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       Unnamed: 0           artist_name             track_name  release_date  \\\n",
       "0           13923               santana                  wham!          1978   \n",
       "1           14958          marvin sease   show me what you got          1991   \n",
       "2           15045  the robert cray band             1040 blues          1993   \n",
       "3           15691             carl sims      it's just a party          2004   \n",
       "4           16322          rufus thomas       sixty minute man          2011   \n",
       "...           ...                   ...                    ...           ...   \n",
       "11973       26756              newsboys  your love never fails          2011   \n",
       "11974       26866        mumford & sons      where are you now          2012   \n",
       "11975       26875           snow patrol           just say yes          2013   \n",
       "11976       27024               starset              carnivore          2014   \n",
       "11977       23435           ronnie self             bop-a-lena          1955   \n",
       "\n",
       "       genre                                             lyrics  len   valence  \n",
       "0      blues  cold chamber smoke kush gettin higher plane sw...  198  0.857791  \n",
       "1      blues  public service announcement weezy baby best ra...  198  0.794930  \n",
       "2      blues  gotta rapper today forget fuck smokin brain ce...  198  0.613561  \n",
       "3      blues  girls knock ghetto ballin real thugs unngghhhh...  198  0.705276  \n",
       "4      blues  diddle devil yabba double walk feel body feel ...  198  0.651690  \n",
       "...      ...                                                ...  ...       ...  \n",
       "11973   rock  ohhh separate away fail know mistake cause fai...   61  0.550701  \n",
       "11974   rock  come hear walk city streets say word finally e...   61  0.101401  \n",
       "11975   rock  run ways want stay okay pretend tell today han...   61  0.236397  \n",
       "11976   rock  life know inside beast grow wait chew rope cha...   61  0.025556  \n",
       "11977   rock  gogalgo bopalena bopalena bopalena bopalena ye...   60  0.968054  \n",
       "\n",
       "[11978 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPLVlvzGqG9t"
   },
   "source": [
    "## 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7xogmO5pY5_"
   },
   "source": [
    "__Lowercase__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QiDCvtSEJwW3"
   },
   "outputs": [],
   "source": [
    "# Convert all data of lyrics to string type.\n",
    "df['lyrics'] = df['lyrics'].apply(str)\n",
    "\n",
    "# Convert all string of lyrics to lowercase.\n",
    "df['lyrics'] = df['lyrics'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YaNEIDSpijq"
   },
   "source": [
    " __Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-Hbewgleps_K"
   },
   "outputs": [],
   "source": [
    "# Tokenize text from lyrics. \n",
    "df['tokenized'] = df['lyrics'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzzI39TcptxE"
   },
   "source": [
    "__Remove punctuations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9-Fnx3CTp1i_"
   },
   "outputs": [],
   "source": [
    "# All punctuations\n",
    "punc_marks = list(string.punctuation)\n",
    "# Remove all punctuations.\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [word for word in x if word not in punc_marks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mebHZRsrqBkU"
   },
   "source": [
    "__Remove stopwords__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tbxDo4tVIrYU"
   },
   "outputs": [],
   "source": [
    "# All stopwords of nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Customized list of stop words.\n",
    "stopwords.extend([\"'m\", \"'s\", \"'d\", \"hi\", \"im\", \"wa\", \"n't\", \"'get\", \"'ll\", \"'re\", \"'ve\", \"get\", \"still\", \"mmm\", \"ooh\", \"oooh\", \"yah\", \"yeh\",\"mmm\", \"hmm\"])\n",
    "\n",
    "# Remove the stop words from the dataset and save the result to new column. \n",
    "df['cleaned_stopwords'] = df['tokenized'].apply(lambda x: [word for word in x if word not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vGIOfVEqbVB"
   },
   "source": [
    "__Lemmatize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "iX5UYCkBKnp5",
    "outputId": "ab987ee2-8ce3-448b-ae4d-d881adc03483"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f90a4d6f-9e4b-48cb-b87c-c0dddcaeab00\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>len</th>\n",
       "      <th>valence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cleaned_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>lyrics_lemmatized</th>\n",
       "      <th>lyrics_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13923</td>\n",
       "      <td>santana</td>\n",
       "      <td>wham!</td>\n",
       "      <td>1978</td>\n",
       "      <td>blues</td>\n",
       "      <td>cold chamber smoke kush gettin higher plane sw...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.857791</td>\n",
       "      <td>[cold, chamber, smoke, kush, gettin, higher, p...</td>\n",
       "      <td>[cold, chamber, smoke, kush, gettin, higher, p...</td>\n",
       "      <td>[(cold, a), (chamber, n), (smoke, v), (kush, n...</td>\n",
       "      <td>[cold, chamber, smoke, kush, gettin, high, pla...</td>\n",
       "      <td>cold chamber smoke kush gettin high plane swan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14958</td>\n",
       "      <td>marvin sease</td>\n",
       "      <td>show me what you got</td>\n",
       "      <td>1991</td>\n",
       "      <td>blues</td>\n",
       "      <td>public service announcement weezy baby best ra...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.794930</td>\n",
       "      <td>[public, service, announcement, weezy, baby, b...</td>\n",
       "      <td>[public, service, announcement, weezy, baby, b...</td>\n",
       "      <td>[(public, a), (service, n), (announcement, n),...</td>\n",
       "      <td>[public, service, announcement, weezy, baby, b...</td>\n",
       "      <td>public service announcement weezy baby best ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15045</td>\n",
       "      <td>the robert cray band</td>\n",
       "      <td>1040 blues</td>\n",
       "      <td>1993</td>\n",
       "      <td>blues</td>\n",
       "      <td>gotta rapper today forget fuck smokin brain ce...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.613561</td>\n",
       "      <td>[got, ta, rapper, today, forget, fuck, smokin,...</td>\n",
       "      <td>[got, ta, rapper, today, forget, fuck, smokin,...</td>\n",
       "      <td>[(got, v), (ta, a), (rapper, n), (today, n), (...</td>\n",
       "      <td>[get, ta, rapper, today, forget, fuck, smokin,...</td>\n",
       "      <td>get ta rapper today forget fuck smokin brain c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f90a4d6f-9e4b-48cb-b87c-c0dddcaeab00')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f90a4d6f-9e4b-48cb-b87c-c0dddcaeab00 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f90a4d6f-9e4b-48cb-b87c-c0dddcaeab00');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Unnamed: 0           artist_name            track_name  release_date  \\\n",
       "0       13923               santana                 wham!          1978   \n",
       "1       14958          marvin sease  show me what you got          1991   \n",
       "2       15045  the robert cray band            1040 blues          1993   \n",
       "\n",
       "   genre                                             lyrics  len   valence  \\\n",
       "0  blues  cold chamber smoke kush gettin higher plane sw...  198  0.857791   \n",
       "1  blues  public service announcement weezy baby best ra...  198  0.794930   \n",
       "2  blues  gotta rapper today forget fuck smokin brain ce...  198  0.613561   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [cold, chamber, smoke, kush, gettin, higher, p...   \n",
       "1  [public, service, announcement, weezy, baby, b...   \n",
       "2  [got, ta, rapper, today, forget, fuck, smokin,...   \n",
       "\n",
       "                                   cleaned_stopwords  \\\n",
       "0  [cold, chamber, smoke, kush, gettin, higher, p...   \n",
       "1  [public, service, announcement, weezy, baby, b...   \n",
       "2  [got, ta, rapper, today, forget, fuck, smokin,...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(cold, a), (chamber, n), (smoke, v), (kush, n...   \n",
       "1  [(public, a), (service, n), (announcement, n),...   \n",
       "2  [(got, v), (ta, a), (rapper, n), (today, n), (...   \n",
       "\n",
       "                                   lyrics_lemmatized  \\\n",
       "0  [cold, chamber, smoke, kush, gettin, high, pla...   \n",
       "1  [public, service, announcement, weezy, baby, b...   \n",
       "2  [get, ta, rapper, today, forget, fuck, smokin,...   \n",
       "\n",
       "                                      lyrics_cleaned  \n",
       "0  cold chamber smoke kush gettin high plane swan...  \n",
       "1  public service announcement weezy baby best ra...  \n",
       "2  get ta rapper today forget fuck smokin brain c...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to attain part of speech of words.\n",
    "def determine_wordnet_speech(word_tag):\n",
    "    if word_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif word_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif word_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Add part of speech tags and save the result to new column.\n",
    "df['pos_tags'] = df['cleaned_stopwords'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "# Function to get part of speech in WordNet format.\n",
    "df['pos_tags'] = df['pos_tags'].apply(lambda x: [(word, determine_wordnet_speech(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "# Lemmatize words and save the result to new column.\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "df['lyrics_lemmatized'] = df['pos_tags'].apply(lambda x: [word_lemmatizer.lemmatize(word, tag) for word, tag in x])\n",
    "\n",
    "# Convert list to string datatype. \n",
    "df['lyrics_cleaned'] = [' '.join(map(str,l)) for l in df['lyrics_lemmatized']]\n",
    "\n",
    "# Check few rows.\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgRLqHwsQ-zO"
   },
   "source": [
    "__Encode label__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4hPu3824Oe5D"
   },
   "outputs": [],
   "source": [
    "# Transform valence to sentiment labels\n",
    "df['sentiment'] = df['valence'].apply(create_sentiment_from_valence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtgZKPCHRJK4"
   },
   "source": [
    "__Data splitting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sj9aEs3hRKHg"
   },
   "outputs": [],
   "source": [
    "# Extract labels \n",
    "y = df['sentiment']\n",
    "# Extract independent variables\n",
    "X = df['lyrics_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVI55z7KtqnR"
   },
   "source": [
    "## 5. Sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N20gw_aZuAgw"
   },
   "source": [
    "### 5.1 RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl8Vwoy9uV1m"
   },
   "source": [
    "### 5.2 LTSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwymy2e_psXO"
   },
   "source": [
    "__Vectorization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uumF0TgiVbsD",
    "outputId": "16ca9d7f-4006-4cb6-80b0-fc6d6c4ea915"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40422"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the length of our vocabulary\n",
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "# Create a dict of word and index from the list of sentences. Required before texts_to_sequences\n",
    "word_tokenizer.fit_on_texts(X)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WfDNNjvMVbzT"
   },
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "all_lyrics = X.values\n",
    "longest_lyrics = max(all_lyrics, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_lyrics))\n",
    "\n",
    "# texts_to_sequences: Transforms each text in texts to a sequence of integers (integers = index of word by fit_on_texts)\n",
    "padded_lyrics = pad_sequences(\n",
    "    word_tokenizer.texts_to_sequences(all_lyrics),\n",
    "    length_long_sentence, \n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s5aRRqZdWjq2"
   },
   "outputs": [],
   "source": [
    "# Split dataset into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_lyrics, y, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6nZsus_pztE"
   },
   "source": [
    "__Modelling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7qePTRr8tEBX"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  with tf.device(device_name):\n",
    "    model = Sequential()\n",
    "    # embedding\n",
    "    hp_vector_size = hp.Int('vector_size', \n",
    "                            min_value=50, \n",
    "                            max_value=200, \n",
    "                            step=50)\n",
    "    model.add(\n",
    "        Embedding(input_dim=vocab_length,\n",
    "                  output_dim=hp_vector_size,\n",
    "                  input_length=length_long_sentence))\n",
    "\n",
    "    # first lstm\n",
    "    hp_lstm_units1 = hp.Int('lstm_units1', \n",
    "                            min_value=256, \n",
    "                            max_value=320, \n",
    "                            step=32)\n",
    "    model.add(LSTM(hp_lstm_units1, return_sequences=True))\n",
    "\n",
    "    # drop out\n",
    "    hp_dropout_rate = hp.Float('dropout_rate', \n",
    "                               min_value=0.4, \n",
    "                               max_value=0.8, \n",
    "                               step=0.2)\n",
    "    model.add(Dropout(hp_dropout_rate))\n",
    "\n",
    "    # second lstm\n",
    "    hp_lstm_units2 = hp.Int('lstm_units2', \n",
    "                            min_value=128, \n",
    "                            max_value=256, \n",
    "                            step=32)\n",
    "    model.add(LSTM(hp_lstm_units2))\n",
    "\n",
    "    # dense layer\n",
    "    model.add(Dense(units = 100, activation='relu'))\n",
    "    model.add(Dense(units = 50, activation='relu'))\n",
    "    model.add(Dense(units = 25, activation='relu'))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    \n",
    "    # compile\n",
    "    hp_learning_rate = hp.Choice('learning_rate', \n",
    "                                 values=[1e-3, 1e-2])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaVqNiTrl8Zu",
    "outputId": "18ac8266-17f7-467d-d683-afc0b223aac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 Complete [00h 02m 27s]\n",
      "val_accuracy: 0.4023372232913971\n",
      "\n",
      "Best val_accuracy So Far: 0.4036727845668793\n",
      "Total elapsed time: 00h 46m 18s\n",
      "\n",
      "Search: Running Trial #23\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "150               |100               |vector_size\n",
      "288               |288               |lstm_units1\n",
      "0.4               |0.8               |dropout_rate\n",
      "128               |128               |lstm_units2\n",
      "0.01              |0.01              |learning_rate\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "4                 |4                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "173/281 [=================>............] - ETA: 1:56 - loss: 1.0636 - accuracy: 0.3960"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     factor=3,\n",
    "                     max_epochs=100,\n",
    "                     directory=\"model_trials_1\",\n",
    "                     project_name=\"emotion_detector_1\",\n",
    "                     overwrite = True\n",
    "                     )\n",
    "                     \n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(X_train, \n",
    "             y_train, \n",
    "             epochs=50,\n",
    "             validation_data=(X_test, y_test), \n",
    "             callbacks=[stop_early]\n",
    "             )\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mvg1pQ_pWXjW"
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NHnT8h0PtILn"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "# prediction2 = []\n",
    "# for i in prediction:\n",
    "#   max_value = max(i)\n",
    "#   prediction2.append(list(i).index(max_value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
